# Optimizing Dataproc Monitor

- In Google Cloud, you can use Cloud Logging and Cloud Monitoring to view and customize logs, and to monitor jobs and resources.
- The best way to find what error caused a Spark job failure is to look at the driver output and the logs generated by the Spark executioners
- output is also stored in the Cloud storage bucket of the Dataproc cluster.
- In a Dataproc cluster, Yarn is configured to collect all these logs by default, and they're available in Cloud Logging.
- To find logs faster, you can create and use your own labels for each cluster or for each Dataproc job.
- Cloud Monitoring can monitor the cluster's CPU, disk, network usage and Yarn resources.
- Dataproc runs on top of Compute Engine. If you want to visualize CPU usage, disk IO or networking metrics in a chart, you need to select a Compute Engine VM instance as the resource type, and then filter by the cluster name.